import torch
import fakequant    # å‡é‡åŒ–æ›¿æ¢ä»£ç 
from datasets import load_dataset
from evaluate import Evaluator
from newLoader import LlmModelLoader  # âœ… ä½ åˆšå†™çš„æ¨¡å‹åŠ è½½ç±»


def run_inference(model, tokenizer, prompt, max_new_tokens=50):
    """ç»™å®šæ¨¡å‹ + tokenizer + promptï¼Œè¿è¡Œæ¨ç†å¹¶è¿”å›æ–‡æœ¬"""
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    model.config.use_cache = True  # å¼ºåˆ¶å¯ç”¨ç¼“å­˜
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=False,  # ä¸é‡‡æ ·ï¼Œä¿è¯å¯é‡å¤æ€§
            use_cache=True
        )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)


def main():
    # === 1ï¸âƒ£ é€‰æ‹©æ¨¡å‹è·¯å¾„ ===
    model_name = "/root/autodl-tmp/model/Qwen2.5-1.5B-Instruct"
    access_token = None  # å¦‚æœæ˜¯ gated æ¨¡å‹ï¼ˆæ¯”å¦‚ LLaMA3ï¼‰å°±å¡« token

    # === 2ï¸âƒ£ ä½¿ç”¨è‡ªå®šä¹‰åŠ è½½å™¨åŠ è½½æ¨¡å‹ ===
    print(f"ğŸ”„ Loading original model {model_name} ...")
    loader = LlmModelLoader(
        model_name_or_path=model_name,
        access_token=access_token,
        dtype="float32",
        max_length=2048
    )
    model = loader.get_model().cuda()
    tokenizer = loader.get_tokenizer()

    # === 3ï¸âƒ£ å®šä¹‰ prompt ===
    prompt = "Please tell me a story."

    # === 4ï¸âƒ£ åŸå§‹æ¨¡å‹æ¨ç† ===
    print("ğŸš€ Running ORIGINAL model inference...")
    output_orig = run_inference(model, tokenizer, prompt)
    print("\n=== âœ… Original Model Output ===")
    print(output_orig)
    
    
    # === 5ï¸âƒ£ æµ‹è¯•å›°æƒ‘åº¦ ===
    dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')
    evaluator = Evaluator(dataset, tokenizer, device="cuda",
                          n_samples=20, ctx_len=2048, stride=2048)
    ppl_fp16 = evaluator.evaluate(model)
    print(f"Original model (fp16) perplexity: {ppl_fp16}")
    
    # === 6ï¸âƒ£ æ›¿æ¢ä¸ºå‡é‡åŒ–çº¿æ€§å±‚ï¼ˆFakeQuantLinearï¼‰ ===
    print("\nğŸ”„ Applying FakeQuant W8A8 (per_channel weight, per_token activation)...")
    model_q = fakequant.quantize_qwen(
        model,
        weight_quant="per_channel",  # æƒé‡é‡åŒ–æ–¹å¼
        act_quant="per_token",       # æ¿€æ´»é‡åŒ–æ–¹å¼
        w_bits=8,                    # æƒé‡é‡åŒ– 
        a_bits=8,                    # æ¿€æ´»é‡åŒ– 
        kv_bits=8,                   # kv-cacheé‡åŒ–
        quantize_kv=True             # æ˜¯å¦kv-cacheé‡åŒ–
    ).cuda()
 
    # === 7ï¸âƒ£ é‡åŒ–åæ¨¡å‹æ¨ç† ===
    print("ğŸš€ Running QUANTIZED model inference (W8A8)...")
    output_quant = run_inference(model_q, tokenizer, prompt)
    print("\n=== âœ… Quantized Model Output ===")
    print(output_quant)
    
    # === 8ï¸âƒ£ é‡åŒ–åå›°æƒ‘åº¦ ===
    ppl_quant = evaluator.evaluate(model_q)
    print(f"Quantized model perplexity: {ppl_quant}")
    

if __name__ == "__main__":
    main()
