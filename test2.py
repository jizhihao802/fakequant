import torch
import fakequant    # å‡é‡åŒ–æ›¿æ¢ä»£ç 
from datasets import load_dataset
from evaluate import Evaluator
from newLoader import LlmModelLoader  # âœ… æ¨¡å‹åŠ è½½ç±»
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer

def run_inference(model, tokenizer, prompt, max_new_tokens=512):
    """ç»™å®šæ¨¡å‹ + tokenizer + promptï¼Œè¿è¡Œæ¨ç†å¹¶è¿”å›æ–‡æœ¬"""
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    model.config.use_cache = True  # å¼ºåˆ¶å¯ç”¨ç¼“å­˜
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=False,  # ä¸é‡‡æ ·ï¼Œä¿è¯å¯é‡å¤æ€§
            use_cache=True
        )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)


def main():
    # === 1ï¸âƒ£ é€‰æ‹©æ¨¡å‹è·¯å¾„ ===
    model_name = "/root/autodl-tmp/model/qwen3-4b"
    access_token = None  # å¦‚æœæ˜¯ gated æ¨¡å‹ï¼ˆæ¯”å¦‚ LLaMA3ï¼‰å°±å¡« token

    # === 2ï¸âƒ£ ä½¿ç”¨è‡ªå®šä¹‰åŠ è½½å™¨åŠ è½½æ¨¡å‹ ===
    print(f"ğŸ”„ Loading original model {model_name} ...")
    
    loader = LlmModelLoader(
        model_name_or_path=model_name,
        access_token=access_token,
        dtype="float32",
        max_length=4096
    )
    model = loader.get_model().cuda()
    tokenizer = loader.get_tokenizer()
    
    #model = AutoModelForCausalLM.from_pretrained("/root/autodl-tmp/model/qwen3-4b").cuda()
    #tokenizer = AutoTokenizer.from_pretrained("/root/autodl-tmp/model/qwen3-4b")
    # === 3ï¸âƒ£ æ·»åŠ ï¼šè¯»å–txtæ–‡ä»¶ï¼Œæ‹¼æ¥prompt ===
    user_prompt = "\nä½ æ˜¯ä¸€ä½ç§˜ä¹¦ï¼Œé˜…è¯»ä»¥ä¸Šçš„ä¼šè®®å†…å®¹ï¼Œè´Ÿè´£ç¼–å†™è§„èŒƒçš„ä¼šè®®çºªè¦ã€‚\n\nè¦æ±‚ï¼š\n1. å…¨é¢æç‚¼ä¼šè®®è¦ç‚¹ï¼Œç¡®ä¿ä¸é—æ¼ä»»ä½•é‡è¦ç»†èŠ‚ã€‚\n2. ä¸¥æ ¼æŒ‰ç…§åˆ—è¡¨å½¢å¼è¾“å‡ºä¼šè®®çºªè¦ï¼Œç¡®ä¿æ ¼å¼è§„èŒƒã€‚\n\nè¾“å‡ºæ ¼å¼ï¼šæ•°å­—åºå·åè·Ÿéšçš„æ˜¯ä½ æå–çš„ä¼šè®®è¦ç‚¹ï¼ŒçŸ­æ¨ªâ€œ-â€åè·Ÿéšçš„æ˜¯è¯¥è¦ç‚¹çš„ç»†èŠ‚\n1. xxxxx\n\t- yyyyy\n\t- yyyyyyy \n2. xxxxxxx\n...\n"
    with open("SpinQuant/meeting.txt", "r", encoding="utf-8") as f:
        wiki_text = f.read().strip()
    
    # ç»Ÿè®¡ token æ•°
    tokens = tokenizer(wiki_text, return_tensors="pt")  # è½¬æˆ PyTorch tensor
    #token_count = len(tokens["input_ids"][0])
    #print("Token æ•°:", token_count)
    
    
    # === æˆªå–ä¸€éƒ¨åˆ† ===
    max_tokens = 3500 - 110  # ä½ è¦è®¾å®šçš„æœ€å¤§é•¿åº¦ï¼Œæ¯”å¦‚æ¨¡å‹ max_length
    truncated_ids = tokens["input_ids"][0, :max_tokens]  # æˆªæ–­
    truncated_text = tokenizer.decode(truncated_ids, skip_special_tokens=True)

    long_text = truncated_text + user_prompt
    short_text = "What is large language model?"
    
    # === 4ï¸âƒ£ åŸå§‹æ¨¡å‹æ¨ç† ===
    print("ğŸš€ Running ORIGINAL model inference...")
    output_orig = run_inference(model, tokenizer, long_text)
    print("\n=== âœ… Original Model Output ===")
    print(output_orig)
    
    # === 6ï¸âƒ£ æ›¿æ¢ä¸ºå‡é‡åŒ–çº¿æ€§å±‚ï¼ˆFakeQuantLinearï¼‰ ===
    print("\nğŸ”„ Applying FakeQuant W8A8 (per_channel weight, per_token activation)...")
    model_q = fakequant.quantize_qwen3(
        model,
        weight_quant="per_channel",  # æƒé‡é‡åŒ–æ–¹å¼
        act_quant="per_token",       # æ¿€æ´»é‡åŒ–æ–¹å¼
        w_bits=8,                    # æƒé‡é‡åŒ– 
        a_bits=8,                    # æ¿€æ´»é‡åŒ– 
        kv_bits=8,                   # kv-cacheé‡åŒ–
        quantize_kv=True             # æ˜¯å¦kv-cacheé‡åŒ–
    ).cuda()
 
    # === 7ï¸âƒ£ é‡åŒ–åæ¨¡å‹æ¨ç† ===
    print("ğŸš€ Running QUANTIZED model inference (W8A8)...")
    output_quant = run_inference(model_q, tokenizer, long_text)
    print("\n=== âœ… Quantized Model Output ===")
    print(output_quant)
    
if __name__ == "__main__":
    main()
